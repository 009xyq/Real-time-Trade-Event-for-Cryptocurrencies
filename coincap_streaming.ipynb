{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7146cf27-c6e9-4c57-8a8e-2f2a3fd70467",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read data from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad8381ae-b7a0-446f-9ef6-bcaa65e129ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Define Kafka configurations\n",
    "kafka_brokers = \"b-1.detrainingmsk.66lq6h.c10.kafka.us-east-1.amazonaws.com:9092\"\n",
    "kafka_topic = 'coincap_trade'\n",
    "\n",
    "\n",
    "df = (spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option('inferSchema', True)\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"latest\") \n",
    "    .load()\n",
    "     )\n",
    "\n",
    "df1=df.select(col(\"value\").cast(\"string\"))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84cc1f2e-8be8-4e94-9cbc-1b0be7833dc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"exchange\", StringType()),\n",
    "    StructField(\"base\", StringType()),\n",
    "    StructField(\"quote\", StringType()),\n",
    "    StructField(\"direction\", StringType()),\n",
    "    StructField(\"price\", DoubleType()),\n",
    "    StructField(\"volume\", DoubleType()),\n",
    "    StructField(\"timestamp\", LongType()),\n",
    "    StructField(\"priceUsd\", DoubleType())\n",
    "  ])\n",
    "\n",
    "df2 = df1.select(\n",
    "      from_json(col(\"value\"), json_schema).alias(\"record\")\n",
    ")\n",
    "\n",
    "df3 = (df2\n",
    "   .withColumn('readable_time', from_unixtime(col(\"timestamp\")/1000))\n",
    "   .filter(\"priceUsd is not NULL\")\n",
    "   .withColumn(\"readable_time\", from_utc_timestamp(from_unixtime(col(\"timestamp\")/1000, \"yyyy-MM-dd HH:mm:ss\"), \"UTC\")) \n",
    ")\n",
    "\n",
    "tumbling_window = (df3\n",
    "   .withWatermark(\"readable_time\", \"5 minutes\")\n",
    "   .withColumn('amount', col('volume') * col('priceUsd'))\n",
    "   .groupBy(\"base\", window(\"readable_time\", \"1 minute\")) \n",
    "   .agg(sum('volume').alias('total_volume'), sum('amount').alias('total_amount'), avg(\"priceUsd\").alias(\"average_price\"))\n",
    "   .withColumn('actual_avg', col('total_amount') / col('total_volume'))\n",
    "   .orderBy(\"base\", \"window\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26af30e7-1cf9-46ab-9c7c-4fc201ef610b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Write result to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c7e2682-c02c-4092-a166-c9caf03eda6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pyspark.sql.streaming.query.StreamingQuery\n",
    "tumbling_window_streaming_query = (\n",
    "  tumbling_window.writeStream\n",
    "  .queryName(\"events_per_tumbling_window\")\n",
    "  .trigger(processingTime='1 minute')\n",
    "  .format(\"memory\")\n",
    "  .outputMode(\"complete\")\n",
    "  .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f5f7764-fc69-4d85-92b1-aa38fea1ac95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Query the result in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8c49e64-1281-4daf-a199-2e3a21d438e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Run index {i}:\")\n",
    "    table_df = spark.sql(\"SELECT * FROM events_per_sliding_window ORDER BY base, window\")\n",
    "\n",
    "    window_size = expr(\"INTERVAL 6 MINUTES\")\n",
    "\n",
    "   price_trend_df = (table_df\n",
    "        .withColumn(\"prev_avg\", lag('actual_avg').over(windowSpec))\n",
    "        .withColumn(\"price_trend\", when(col(\"acturl_avg\") > col(\"prev_avg\"), \"up\")\n",
    "                                .when(col(\"acturl_avg\") < col(\"prev_avg\"), \"down\")\n",
    "                                .otherwise(\"null\"))\n",
    "        .withColumn(\"current_time\", current_timestamp())\n",
    "        .withColumn(\"current_time_minus_5m\", col(\"current_time\") - window_size)\n",
    "        .filter(col(\"window.start\") > col(\"current_time_minus_5m\"))\n",
    "    )\n",
    "    price_trend_df.select('base', 'window', 'total_volume', 'actual_avg', 'price_trend').show(20,truncate=False)\n",
    "    time.sleep(60)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write result to a Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_chkpoint_loc = 's3a://asc-de-training-destination-s3/roger/checkpint/coincap_streaming'\n",
    "table_path = 's3a://asc-de-training-destination-s3/roger/data/coincap_streaming_result'\n",
    "\n",
    "tumbling_window_streaming_query_delta = (\n",
    "    price_trend_df.writeStream\n",
    "    .format('delta')\n",
    "    .outputMode(\"complete\")\n",
    "    .trigger(processingTime='1 minute')\n",
    "    .option(\"checkpointLocation\", stream_chkpoint_loc)\n",
    "    .option(\"path\", table_path)\n",
    "    .toTable(\"coincap_streaming_result\")\n",
    "    .start()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "coincap_streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
